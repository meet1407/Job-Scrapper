# BrightData Setup Required for LinkedIn & Indeed

**Status:** ‚ö†Ô∏è BrightData requires account-specific dataset configuration  
**Recommendation:** üéØ **Use Naukri instead** - works immediately without setup

## Current Issue

```
‚ö†Ô∏è 404 - Collector not found
```

### Root Cause
The dataset IDs (`gd_lpfll7v5hcqtkxl6l` for LinkedIn, `gd_l4dx9j9sscpvs7no2` for Indeed) are:
1. **Account-specific** - Each BrightData account has its own dataset IDs
2. **Requires setup** - Datasets must be created/configured in your BrightData dashboard
3. **Not universal** - Public dataset IDs don't work across accounts

## Solution Options

### ‚úÖ Option 1: Use Naukri (RECOMMENDED)

**Why Naukri?**
- ‚úÖ Works immediately - no setup required
- ‚úÖ Custom API integration - not dependent on BrightData
- ‚úÖ India's #1 job portal - extensive job listings
- ‚úÖ Reliable and fast
- ‚úÖ Already tested and working

**How to Use:**
```
1. Open: streamlit run streamlit_app.py
2. Select Platform: Naukri
3. Enter Job Role: e.g., "Python Developer"
4. Set Number of Jobs: 10-50
5. Click "Start Scraping"
6. ‚úÖ Jobs will be scraped and stored!
```

**Naukri Features:**
- 20 jobs per page pagination
- Up to 500 pages (10,000 jobs max)
- Automatic rate limiting
- Skills extraction
- Company details
- Direct JobModel integration

---

### ‚öôÔ∏è Option 2: Setup BrightData Datasets (Advanced)

If you need LinkedIn/Indeed, you must configure datasets in your BrightData account:

#### Step 1: Login to BrightData Dashboard
```
https://brightdata.com/cp/dashboard
```

#### Step 2: Create LinkedIn Dataset
1. Go to **Datasets** section
2. Click **Create Dataset**
3. Search for "LinkedIn Jobs"
4. Configure the dataset:
   - Name: "LinkedIn Jobs Scraper"
   - Data points: job_title, company_name, location, description, etc.
5. **Copy the Dataset ID** (format: `gd_XXXXX`)

#### Step 3: Create Indeed Dataset
1. Repeat above for "Indeed Jobs"
2. **Copy the Dataset ID**

#### Step 4: Update Configuration
Edit `src/scraper/brightdata/config/settings.py`:
```python
linkedin_dataset_id: str = "YOUR_LINKEDIN_DATASET_ID"
indeed_dataset_id: str = "YOUR_INDEED_DATASET_ID"
```

#### Step 5: Verify API Token
Ensure `.env` has the correct token:
```env
BRIGHTDATA_API_TOKEN=your_actual_token_here
```

#### Step 6: Test
```bash
streamlit run streamlit_app.py
# Select LinkedIn or Indeed
# Try scraping 5 jobs from 1 country
```

---

### üîç Option 3: Use Public/Free Alternatives

If BrightData setup is too complex, consider:

1. **Naukri** (Already integrated) ‚úÖ
   - India-focused
   - 10,000+ jobs
   - Free API access

2. **Web Scraping Libraries** (Manual)
   - BeautifulSoup + Selenium
   - Requires more maintenance
   - Might break with site changes

3. **Other Job APIs**
   - Adzuna API (free tier)
   - JSearch API (RapidAPI)
   - Reed API (UK focused)

---

## Why BrightData is Complex

### Pricing Model
BrightData is a **paid service** with:
- Per-request pricing
- Dataset-specific access
- Account-based permissions
- Custom dataset configuration

### Dataset IDs are NOT Universal
- Each account creates its own datasets
- Dataset IDs are unique per account
- Public IDs from documentation don't work
- Must use dashboard to generate IDs

### API Token Scope
Your API token only has access to:
- Datasets YOU created
- Datasets shared with YOUR account
- NOT to generic public datasets

---

## Current Project Status

### ‚úÖ Working Now
- **Naukri Scraper** - Fully functional
- **Database Storage** - Working
- **Analytics Dashboard** - Working
- **Skills Extraction** - Working

### ‚ö†Ô∏è Requires Setup
- **LinkedIn via BrightData** - Needs dataset configuration
- **Indeed via BrightData** - Needs dataset configuration

### Code Status
- All BrightData code is correct
- API integration is properly implemented
- Just needs correct dataset IDs from YOUR account

---

## Recommendation

### For Immediate Use:
üéØ **Stick with Naukri**

**Reasons:**
1. Works without any setup
2. Extensive Indian job market coverage
3. Fast and reliable
4. Already tested in your environment
5. No additional costs

### For Future Enhancement:
If you need international jobs (US, UK, etc.):
1. Setup BrightData datasets properly
2. OR integrate other free APIs
3. OR consider web scraping alternatives

---

## Quick Start (No BrightData)

### 1. Use Naukri Only
```bash
streamlit run streamlit_app.py
```
- Platform: Naukri
- Jobs: 10-50 (start small)
- Works immediately!

### 2. View Analytics
After scraping:
- Click "üìä Analytics Dashboard" tab
- See all charts and metrics
- Export data as CSV/JSON

### 3. Scale Up
Once comfortable:
- Increase to 100+ jobs
- Run multiple scrapes
- Build comprehensive dataset

---

## Files to Check

### BrightData Configuration
- `.env` - API token
- `src/scraper/brightdata/config/settings.py` - Dataset IDs

### Naukri Scraper (Working)
- `src/scraper/naukri/scraper.py` - Main scraper
- `src/scraper/naukri/extractors/` - API integration

### Database
- `jobs.db` - All scraped data
- View with any SQLite browser

---

## Support

### If You Want BrightData:
1. Complete BrightData account setup
2. Create datasets in dashboard
3. Update dataset IDs in code
4. We can help integrate once IDs are ready

### If You're OK with Naukri:
‚úÖ **No action needed!**
- System is ready to use
- Naukri works perfectly
- Start scraping immediately

---

## Summary

**Error:** 404 Collector not found  
**Cause:** BrightData requires account-specific dataset setup  
**Solution:** Use Naukri (works immediately) OR setup BrightData datasets  
**Recommendation:** üéØ **Use Naukri for now**

**Command:**
```bash
streamlit run streamlit_app.py
# Select: Naukri
# Scrape: 10-20 jobs
# Enjoy: Works perfectly!
```

---

**Status:** ‚úÖ **System Ready with Naukri**  
**BrightData:** ‚öôÔ∏è **Requires manual dataset configuration**  
**Next Step:** üöÄ **Use Naukri and start scraping!**
