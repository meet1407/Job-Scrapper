"""
Bulk URL Deduplication - Lightning-fast batch checking
Optimized for 10,000+ URL verification in <1 second
"""
import sqlite3
import logging
from typing import Set

logger = logging.getLogger(__name__)


class BulkURLChecker:
    """Ultra-fast bulk URL existence checker with SQLite optimization"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self._create_optimized_indexes()
    
    def _create_optimized_indexes(self) -> None:
        """Ensure hash-based index for O(1) URL lookups"""
        conn = sqlite3.connect(self.db_path)
        try:
            # Hash index on URL for instant lookups
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_jobs_url_hash 
                ON jobs(url)
            """)
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_job_urls_url_hash 
                ON job_urls(url)
            """)
            conn.commit()
        finally:
            conn.close()
    
    def check_bulk_urls(self, urls: list[str]) -> dict[str, Set[str]]:
        """Check 10,000+ URLs in single query (<1 second)
        
        Returns:
            {
                'existing': set of URLs already in database,
                'new': set of URLs not in database
            }
        """
        if not urls:
            return {'existing': set(), 'new': set()}
        
        url_set = set(urls)  # Remove duplicates from input
        
        log_msg = f"ðŸ” Bulk URL Check: Checking {len(url_set)} URLs against database..."
        print(log_msg)
        logger.info(log_msg)
        
        conn = sqlite3.connect(self.db_path)
        try:
            # Batch query with hash index (O(N) with index)
            placeholders = ','.join('?' * len(url_set))
            cursor = conn.execute(f"""
                SELECT DISTINCT url FROM (
                    SELECT url FROM jobs WHERE url IN ({placeholders})
                    UNION
                    SELECT url FROM job_urls WHERE url IN ({placeholders})
                )
            """, list(url_set) * 2)
            
            existing = {row[0] for row in cursor.fetchall()}
            new = url_set - existing
            
            log_msg = f"âœ… Database Check Complete: {len(existing)} duplicates found, {len(new)} new URLs"
            print(log_msg)
            logger.info(log_msg)
            
            return {'existing': existing, 'new': new}
        finally:
            conn.close()
    
    def get_stats(self, urls: list[str]) -> dict[str, int | float]:
        """Get quick statistics for URL batch"""
        result = self.check_bulk_urls(urls)
        return {
            'total': len(urls),
            'existing': len(result['existing']),
            'new': len(result['new']),
            'duplicate_rate': len(result['existing']) / len(urls) * 100 if urls else 0
        }
