# Source Code Architecture

## Overview
This directory contains the complete job scraping system organized into specialized modules that work together to collect, validate, and analyze job market data.

## System Architecture

```
src/
â”œâ”€â”€ analysis/          # Skill extraction and validation engine
â”œâ”€â”€ config/            # Centralized configuration and reference data
â”œâ”€â”€ db/                # Database operations and data persistence
â”œâ”€â”€ models/            # Data structures and validation rules
â”œâ”€â”€ scraper/           # Web scraping automation (LinkedIn & Naukri)
â””â”€â”€ ui/                # Streamlit dashboard interface
```

## Module Responsibilities

### ğŸ“Š Analysis Module
**Purpose**: Extracts technical skills from job descriptions  
**Key Output**: Validated skill lists matching 749 canonical skills  
**Quality Impact**: Eliminates false positives, ensures accurate analytics

### âš™ï¸ Config Module
**Purpose**: Stores system-wide reference data  
**Contains**: 749 skills, country codes, location mappings  
**Benefit**: Update once, apply everywhere

### ğŸ’¾ Database Module
**Purpose**: Manages SQLite data storage with atomic transactions  
**Key Feature**: Resume capabilityâ€”never lose progress  
**Reliability**: Two-phase commit ensures data integrity

### ğŸ“‹ Models Module
**Purpose**: Defines data structures and validation rules  
**Role**: Powers Gate 1 of triple validation system  
**Protection**: Prevents incomplete data from entering pipeline

### ğŸŒ Scraper Module
**Purpose**: Automated job collection from LinkedIn and Naukri  
**Architecture**: Two-phase (URL collection â†’ Detail extraction)  
**Performance**: 8 concurrent workers with adaptive rate limiting  
**Features**: Anti-detection, real-time deduplication, fallback selectors

### ğŸ–¥ï¸ UI Module
**Purpose**: User-facing Streamlit dashboard  
**Interface**: 3-tab design (Link Scraper, Detail Scraper, Analytics)  
**Experience**: Real-time progress tracking, visual validation gates

## Data Flow

```
1. USER INPUT (via UI)
   â†“
2. SCRAPER collects URLs â†’ stores in DB
   â†“
3. SCRAPER extracts details â†’ validates via MODELS
   â†“
4. ANALYSIS extracts skills â†’ validates against CONFIG
   â†“
5. DB stores validated data atomically
   â†“
6. UI displays analytics from DB
```

## Quality Gates Integration

**Gate 1** (Models): Field validationâ€”required fields, minimum lengths  
**Gate 2** (Analysis): Skill validationâ€”749 canonical skills matching  
**Gate 3** (Database): Atomic storageâ€”all-or-nothing transactions

## Key Design Principles

- **Modularity**: Each module has single, clear responsibility
- **Centralization**: Configuration managed in one place
- **Reliability**: Atomic transactions and resume capability
- **Efficiency**: Concurrent processing with adaptive rate limiting
- **Quality**: Triple validation ensures data accuracy

## For Developers

Each subfolder contains its own README with detailed module documentation. Start with the module most relevant to your work area.

## For Stakeholders

This architecture ensures:
- **Data Quality**: Triple validation catches bad data early
- **Reliability**: System can resume after interruptions
- **Scalability**: Concurrent processing handles large datasets
- **Maintainability**: Modular design simplifies updates
- **Transparency**: Real-time progress visible through UI

---

**Total Modules**: 6 specialized components  
**Total LOC**: ~3,500 lines (excluding tests)  
**Design Philosophy**: EMD (Elegant Modular Design) with â‰¤80 lines per file
